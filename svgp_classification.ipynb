{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(694764, 7) (694764,) (694764,)\n"
     ]
    }
   ],
   "source": [
    "all_clusters = np.loadtxt('all_clusters.txt')\n",
    "all_labs = np.loadtxt('all_labs.txt')\n",
    "all_labs_cp = np.loadtxt('all_labs_cleaned.txt')\n",
    "print(all_clusters.shape, all_labs.shape, all_labs_cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([692025, 7]) torch.Size([692025, 13])\n"
     ]
    }
   ],
   "source": [
    "train_x = all_clusters[all_labs_cp!=-1.]\n",
    "train_y = all_labs_cp[all_labs_cp!=-1.]\n",
    "train_x = torch.tensor(train_x)\n",
    "train_y = torch.tensor(train_y, dtype=torch.int64)\n",
    "\n",
    "train_y = torch.nn.functional.one_hot(train_y)\n",
    "# convert y to one-hot vector for each event\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "# print(train_x[:-10], train_y[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskBernoulliLikelihood(gpytorch.likelihoods.Likelihood):\n",
    "    def forward(self, function_samples, **kwargs):\n",
    "\n",
    "#         prob = torch.exp(function_samples)\n",
    "        prob = function_samples[0]\n",
    "#         print(prob.shape)\n",
    "        output_probs = torch.distributions.Normal(0, 1).cdf(prob)\n",
    "#         out = torch.distributions.Independent(torch.distributions.Bernoulli(probs=output_probs), 1)\n",
    "        out = torch.distributions.Independent(torch.distributions.Bernoulli(logits=output_probs), 1)\n",
    "#         print(out)\n",
    "    \n",
    "        return out  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10, 7])\n"
     ]
    }
   ],
   "source": [
    "num_latents = 10\n",
    "num_tasks = 13\n",
    "input_dim=train_x.shape[-1]\n",
    "num_ind_points = 10\n",
    "\n",
    "class MultitaskGPModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self):\n",
    "        # Let's use a different set of inducing points for each latent function\n",
    "        inducing_points = torch.rand(num_latents, num_ind_points, input_dim)\n",
    "        print(inducing_points.shape)\n",
    "\n",
    "        # We have to mark the CholeskyVariationalDistribution as batch\n",
    "        # so that we learn a variational distribution for each task\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(\n",
    "            inducing_points.size(-2), batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "        # We have to wrap the VariationalStrategy in a LMCVariationalStrategy\n",
    "        # so that the output will be a MultitaskMultivariateNormal rather than a batch output\n",
    "        variational_strategy = gpytorch.variational.LMCVariationalStrategy(\n",
    "            gpytorch.variational.VariationalStrategy(\n",
    "                self, inducing_points, variational_distribution, learn_inducing_locations=True\n",
    "            ),\n",
    "            num_tasks=num_tasks,\n",
    "            num_latents=num_latents,\n",
    "            latent_dim=-1\n",
    "        )\n",
    "\n",
    "        super().__init__(variational_strategy)\n",
    "\n",
    "        # The mean and covariance modules should be marked as batch\n",
    "        # so we learn a different set of hyperparameters\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size([num_latents]))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(batch_shape=torch.Size([num_latents])),\n",
    "            batch_shape=torch.Size([num_latents])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The forward function should be written as if we were dealing with each output\n",
    "        # dimension in batch\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "model = MultitaskGPModel()\n",
    "likelihood = gpytorch.likelihoods.MultitaskGaussianLikelihood(num_tasks=num_tasks)\n",
    "# likelihood = MultitaskBernoulliLikelihood()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "692025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-01d72c4b04c4>:20: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95a7b54878784932a33941e81629ab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1da9f6c076744b88166e862ed75e16f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Minibatch'), FloatProgress(value=0.0, max=1352.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       dtype=torch.float64, grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1151,  0.3458,  0.2847, -0.2168, -0.1188, -0.0336,  0.2197,  0.2375,\n",
      "         0.0897, -0.1114, -0.0834,  0.7626, -0.2344], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1371,  0.2027,  0.2559, -0.0918,  0.0032,  0.0356, -0.0836,  0.0877,\n",
      "         0.0271, -0.0396, -0.1408,  0.2669, -0.0591], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0626,  0.0682,  0.0115,  0.0026,  0.0593,  0.0333, -0.1502,  0.0106,\n",
      "         0.0097,  0.0249, -0.1243, -0.0705,  0.1133], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-0.0045, -0.0089, -0.1151,  0.0582,  0.0401,  0.0282, -0.1409, -0.0271,\n",
      "         0.0237,  0.0672, -0.0612, -0.2296,  0.1459], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-0.0421, -0.0232, -0.1388,  0.0643,  0.0005,  0.0254, -0.1013, -0.0325,\n",
      "         0.0525,  0.0738, -0.0068, -0.2702,  0.1012], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-0.0524, -0.0053, -0.1106,  0.0364, -0.0358,  0.0203, -0.0503, -0.0204,\n",
      "         0.0797,  0.0598,  0.0269, -0.2464,  0.0368], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-0.0429,  0.0253, -0.0785, -0.0041, -0.0636,  0.0104,  0.0037,  0.0028,\n",
      "         0.0966,  0.0323,  0.0425, -0.1918, -0.0126], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([-0.0217,  0.0502, -0.0582, -0.0418, -0.0826, -0.0005,  0.0544,  0.0241,\n",
      "         0.0990,  0.0020,  0.0464, -0.1286, -0.0398], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0044,  0.0645, -0.0441, -0.0682, -0.0943, -0.0133,  0.0902,  0.0428,\n",
      "         0.0911, -0.0331,  0.0461, -0.0742, -0.0544], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0295,  0.0652, -0.0379, -0.0827, -0.1007, -0.0229,  0.1140,  0.0532,\n",
      "         0.0764, -0.0642,  0.0438, -0.0308, -0.0582], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0497,  0.0579, -0.0352, -0.0860, -0.1033, -0.0302,  0.1271,  0.0592,\n",
      "         0.0589, -0.0925,  0.0416,  0.0017, -0.0559], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0633,  0.0451, -0.0293, -0.0802, -0.1012, -0.0342,  0.1292,  0.0622,\n",
      "         0.0410, -0.1152,  0.0387,  0.0237, -0.0525], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0703,  0.0305, -0.0192, -0.0688, -0.0949, -0.0330,  0.1247,  0.0613,\n",
      "         0.0245, -0.1277,  0.0352,  0.0389, -0.0498], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0722,  0.0182, -0.0034, -0.0534, -0.0845, -0.0268,  0.1151,  0.0578,\n",
      "         0.0106, -0.1300,  0.0319,  0.0502, -0.0487], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0713,  0.0104,  0.0188, -0.0362, -0.0708, -0.0153,  0.1030,  0.0510,\n",
      "        -0.0008, -0.1204,  0.0290,  0.0600, -0.0504], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 6.9883e-02,  8.1645e-03,  4.6206e-02, -1.7895e-02, -5.5003e-02,\n",
      "         5.3564e-05,  8.8562e-02,  4.0789e-02, -8.8988e-03, -9.9402e-02,\n",
      "         2.7289e-02,  7.0396e-02, -5.3980e-02], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0696,  0.0110,  0.0767,  0.0008, -0.0386,  0.0168,  0.0725,  0.0278,\n",
      "        -0.0139, -0.0691,  0.0283,  0.0823, -0.0582], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0695,  0.0173,  0.1064,  0.0197, -0.0237,  0.0324,  0.0555,  0.0126,\n",
      "        -0.0145, -0.0323,  0.0331,  0.0967, -0.0614], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0700,  0.0253,  0.1348,  0.0379, -0.0101,  0.0453,  0.0392, -0.0037,\n",
      "        -0.0105,  0.0084,  0.0415,  0.1103, -0.0644], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0724,  0.0336,  0.1605,  0.0550,  0.0011,  0.0535,  0.0249, -0.0192,\n",
      "        -0.0020,  0.0494,  0.0528,  0.1212, -0.0669], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0732,  0.0410,  0.1815,  0.0690,  0.0101,  0.0571,  0.0130, -0.0310,\n",
      "         0.0109,  0.0869,  0.0667,  0.1271, -0.0682], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0724,  0.0460,  0.1985,  0.0798,  0.0172,  0.0567,  0.0038, -0.0379,\n",
      "         0.0267,  0.1197,  0.0808,  0.1266, -0.0692], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0672,  0.0485,  0.2089,  0.0847,  0.0236,  0.0515,  0.0015, -0.0388,\n",
      "         0.0442,  0.1452,  0.0932,  0.1192, -0.0693], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0605,  0.0496,  0.2121,  0.0832,  0.0305,  0.0433,  0.0030, -0.0348,\n",
      "         0.0635,  0.1625,  0.1025,  0.1048, -0.0690], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0567,  0.0500,  0.2088,  0.0757,  0.0369,  0.0333,  0.0092, -0.0260,\n",
      "         0.0823,  0.1713,  0.1062,  0.0845, -0.0679], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0561,  0.0497,  0.1998,  0.0652,  0.0425,  0.0274,  0.0170, -0.0141,\n",
      "         0.0944,  0.1715,  0.1034,  0.0610, -0.0659], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0575,  0.0493,  0.1867,  0.0535,  0.0467,  0.0241,  0.0253, -0.0012,\n",
      "         0.1006,  0.1647,  0.0943,  0.0366, -0.0629], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0618,  0.0483,  0.1714,  0.0449,  0.0488,  0.0230,  0.0332,  0.0091,\n",
      "         0.0975,  0.1519,  0.0792,  0.0152, -0.0580], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0680,  0.0492,  0.1555,  0.0383,  0.0480,  0.0202,  0.0339,  0.0178,\n",
      "         0.0909,  0.1355,  0.0600, -0.0010, -0.0520], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0700,  0.0526,  0.1395,  0.0374,  0.0434,  0.0180,  0.0304,  0.0248,\n",
      "         0.0875,  0.1163,  0.0384, -0.0113, -0.0450], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0745,  0.0574,  0.1239,  0.0383,  0.0364,  0.0188,  0.0251,  0.0296,\n",
      "         0.0900,  0.0958,  0.0164, -0.0154, -0.0362], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0736,  0.0644,  0.1101,  0.0420,  0.0278,  0.0188,  0.0197,  0.0325,\n",
      "         0.0964,  0.0754, -0.0046, -0.0109, -0.0250], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0740,  0.0730,  0.0981,  0.0468,  0.0180,  0.0224,  0.0144,  0.0339,\n",
      "         0.0984,  0.0567, -0.0224,  0.0022, -0.0123], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0696,  0.0858,  0.0878,  0.0513,  0.0102,  0.0286,  0.0092,  0.0362,\n",
      "         0.1010,  0.0399, -0.0365,  0.0229,  0.0010], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0721,  0.1040,  0.0789,  0.0538,  0.0046,  0.0352,  0.0028,  0.0396,\n",
      "         0.1030,  0.0257, -0.0449,  0.0488,  0.0141], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0756,  0.1213,  0.0721,  0.0531,  0.0047,  0.0397,  0.0019,  0.0466,\n",
      "         0.1049,  0.0140, -0.0477,  0.0754,  0.0272], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0806,  0.1398,  0.0667,  0.0523,  0.0118,  0.0424,  0.0043,  0.0553,\n",
      "         0.0964,  0.0060, -0.0440,  0.1003,  0.0374], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0803,  0.1547,  0.0615,  0.0525,  0.0246,  0.0416,  0.0107,  0.0702,\n",
      "         0.0971,  0.0033, -0.0347,  0.1179,  0.0424], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0849,  0.1525,  0.0572,  0.0623,  0.0419,  0.0338,  0.0201,  0.0797,\n",
      "         0.1054,  0.0075, -0.0192,  0.1234,  0.0413], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0850,  0.1397,  0.0553,  0.0733,  0.0569,  0.0290,  0.0299,  0.0741,\n",
      "         0.1379,  0.0164, -0.0005,  0.1155,  0.0329], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.0984, 0.1342, 0.0548, 0.0797, 0.0627, 0.0309, 0.0319, 0.0579, 0.1575,\n",
      "        0.0318, 0.0192, 0.0943, 0.0189], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.1018, 0.1488, 0.0549, 0.0760, 0.0617, 0.0500, 0.0270, 0.0433, 0.1202,\n",
      "        0.0483, 0.0366, 0.0659, 0.0078], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0910,  0.1681,  0.0590,  0.0617,  0.0542,  0.0761,  0.0244,  0.0351,\n",
      "         0.1104,  0.0578,  0.0473,  0.0349, -0.0009], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0844,  0.1777,  0.0657,  0.0461,  0.0393,  0.1064,  0.0280,  0.0614,\n",
      "         0.0898,  0.0611,  0.0477,  0.0141, -0.0032], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0898, 0.1793, 0.0757, 0.0587, 0.0387, 0.0436, 0.0414, 0.0929, 0.1151,\n",
      "        0.0534, 0.0362, 0.0083, 0.0041], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.0859, 0.1609, 0.0849, 0.0694, 0.0438, 0.0180, 0.0672, 0.0887, 0.1206,\n",
      "        0.0363, 0.0189, 0.0321, 0.0140], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.1278, 0.1566, 0.0938, 0.0545, 0.0425, 0.0366, 0.0713, 0.0716, 0.1117,\n",
      "        0.0152, 0.0025, 0.0739, 0.0233], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1042,  0.1952,  0.1031,  0.0549,  0.0235,  0.0832,  0.0552,  0.0526,\n",
      "         0.1324, -0.0023, -0.0093,  0.1098,  0.0256], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.0646,  0.1984,  0.1118,  0.1218,  0.0254,  0.0612,  0.0264,  0.0672,\n",
      "         0.1171, -0.0063, -0.0021,  0.1131,  0.0128], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([ 0.1038,  0.1258,  0.1149,  0.0938,  0.0682, -0.0132,  0.0347,  0.1051,\n",
      "         0.1350,  0.0145,  0.0152,  0.0809,  0.0082], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.1603, 0.1096, 0.1167, 0.0125, 0.0750, 0.0643, 0.0534, 0.0518, 0.1154,\n",
      "        0.0457, 0.0312, 0.0470, 0.0141], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "tensor([0.0630, 0.1858, 0.1064, 0.0800, 0.0363, 0.1111, 0.0733, 0.0418, 0.1158,\n",
      "        0.0718, 0.0251, 0.0609, 0.0251], dtype=torch.float64,\n",
      "       grad_fn=<SelectBackward>)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-01d72c4b04c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mminibatch_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_postfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},\n",
    "], lr=0.1)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO, which essentially just computes the ELBO\n",
    "print(train_y.size(0))\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "losses = []\n",
    "\n",
    "# We use more CG iterations here because the preconditioner introduced in the NeurIPS paper seems to be less\n",
    "# effective for VI.\n",
    "epochs_iter = tqdm.tqdm_notebook(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        model.double()\n",
    "        likelihood.double()\n",
    "        x_batch.double()\n",
    "        y_batch.double()\n",
    "        \n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        losses.append(loss)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predictions = likelihood(output)\n",
    "        print(predictions.mean[0])\n",
    "    \n",
    "    \n",
    "plt.plot(losses)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "fig, axs = plt.subplots(input_dim, (num_tasks-5), figsize=(3 * input_dim, 4 * (num_tasks -5)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "                \n",
    "        predictions = likelihood(model(x_batch))\n",
    "        mean = predictions.mean.ge(0.5).float()\n",
    "        \n",
    "        for xdim in range(7):\n",
    "            for task in range(num_tasks - 5):\n",
    "                ax = axs[xdim][task]\n",
    "                \n",
    "                \n",
    "                ax.plot(x_batch[:, xdim].detach().numpy(), mean[:, task].detach().numpy(), '*b')\n",
    "                ax.plot(x_batch[:, xdim].detach().numpy(), y_batch[:, task].detach().numpy(), 'xr', alpha=0.2)\n",
    "\n",
    "                # Shade in confidence\n",
    "        #         ax.fill_between(\n",
    "        #             test_x[:, xdim].detach().numpy(),\n",
    "        #             lower[:, task].detach().numpy(),\n",
    "        #             upper[:, task].detach().numpy(),\n",
    "        #             alpha=0.5,\n",
    "        #         )\n",
    "                ax.set_ylim([-0.1, 1.1])\n",
    "                ax.legend([ 'Mean', 'Observed Data','Confidence'])\n",
    "                ax.set_title(f'Task {task + 1}')\n",
    "        break\n",
    "\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "maes = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        model.double()\n",
    "        x_batch = x_batch.double()\n",
    "        predictions = likelihood(model(x_batch))\n",
    "        preds = predictions.mean.ge(0.5).float()\n",
    "        \n",
    "#         print(y_batch[0])\n",
    "        if torch.rand(1) > 0.95:\n",
    "#             print(preds[0:5])\n",
    "            print(predictions.mean[0])\n",
    "        \n",
    "        mae = torch.mean(torch.abs(preds - y_batch))\n",
    "#         print(means.shape, preds.shape,y_batch.shape)\n",
    "        maes.append(mae.numpy())\n",
    "        break\n",
    "    \n",
    "maes = np.array(maes)\n",
    "print(maes.mean(), maes.min(), maes.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGP https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Regression_CUDA.html\n",
    "# SVGP CLass https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/Non_Gaussian_Likelihoods.html\n",
    "# DKL Multiclass https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html\n",
    "# Exact Dirichlet https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html?highlight=dirichlet\n",
    "\n",
    "# https://github.com/cornellius-gp/gpytorch/issues/1396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.07692308\n",
    "# tensor([0.1282, 0.1795, 0.0893, 0.0634, 0.0562, 0.0672, 0.0699, 0.0698, 0.0961,\n",
    "#         0.0345, 0.0263, 0.0643, 0.0166], dtype=torch.float64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
