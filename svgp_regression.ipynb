{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_clusters = np.loadtxt('all_clusters.txt')\n",
    "all_labs = np.loadtxt('all_labs.txt')\n",
    "all_labs_cp = np.loadtxt('all_labs_cleaned.txt')\n",
    "print(all_clusters.shape, all_labs.shape, all_labs_cp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = all_clusters[all_labs_cp!=-1.]\n",
    "train_y = all_labs_cp[all_labs_cp!=-1.]\n",
    "train_x = torch.tensor(train_x)\n",
    "train_y = torch.tensor(train_y)\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(train_x[:-10], train_y[:-10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# test_dataset = TensorDataset(test_x, test_y)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpytorch.models import ApproximateGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "\n",
    "class GPModel(ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        \n",
    "        variational_strategy = VariationalStrategy(\n",
    "            self,\n",
    "            inducing_points,\n",
    "            variational_distribution,\n",
    "            learn_inducing_locations=True,\n",
    "        )\n",
    "        super(GPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "inducing_points = train_x[::1000, :]\n",
    "print(inducing_points.shape)\n",
    "model = GPModel(inducing_points=inducing_points)\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    [{'params': model.parameters()},\n",
    "    {'params': likelihood.parameters()},],\n",
    "    lr=0.1,\n",
    ")\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "losses = []\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        model.double()\n",
    "        x_batch = x_batch.double()\n",
    "        y_batch = y_batch.double()\n",
    "        output = model(x_batch)\n",
    "        likelihood.double()\n",
    "        loss = -mll(output, y_batch)\n",
    "        losses.append(loss) \n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "        model.double()\n",
    "        x_batch = x_batch.double()\n",
    "        preds = model(x_batch)\n",
    "        mean = torch.round(model(x_batch).mean)\n",
    "        means = torch.cat([means, mean.cpu()])\n",
    "means = means[1:]\n",
    "print('Test MAE: {}'.format(torch.mean(torch.abs(means - train_y.cpu()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVGP https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/SVGP_Regression_CUDA.html\n",
    "# SVGP CLass https://docs.gpytorch.ai/en/stable/examples/04_Variational_and_Approximate_GPs/Non_Gaussian_Likelihoods.html\n",
    "# DKL Multiclass https://docs.gpytorch.ai/en/stable/examples/06_PyTorch_NN_Integration_DKL/Deep_Kernel_Learning_DenseNet_CIFAR_Tutorial.html\n",
    "# Exact Dirichlet https://docs.gpytorch.ai/en/stable/examples/01_Exact_GPs/GP_Regression_on_Classification_Labels.html?highlight=dirichlet\n",
    "\n",
    "# https://github.com/cornellius-gp/gpytorch/issues/1396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set into eval mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Initialize plots\n",
    "fig, axs = plt.subplots(7, 1, figsize=(4, 3 * 7))\n",
    "    \n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        if torch.cuda.is_available():\n",
    "                x_batch, y_batch = x_batch.cuda(), y_batch.cuda()\n",
    "                \n",
    "        mean = torch.round(model(x_batch).mean)\n",
    "        \n",
    "        for xdim in range(7):\n",
    "            task = 1\n",
    "            ax = axs[xdim]\n",
    "            \n",
    "            ax.plot(x_batch[:, xdim].detach().numpy(), mean.detach().numpy(), '*b')\n",
    "            ax.plot(x_batch[:, xdim].detach().numpy(), y_batch.detach().numpy(), 'xr', alpha=0.5)\n",
    "            ax.legend([ 'Mean', 'Observed Data','Confidence'])\n",
    "            ax.set_title(f'Task {task + 1}')\n",
    "        break\n",
    "\n",
    "fig.tight_layout()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
